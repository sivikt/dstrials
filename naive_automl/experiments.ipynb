{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Problem\n",
    "\n",
    "\n",
    "### Overview\n",
    "One of the key features of AutoML is finding the best predictive model for users’ data, making\n",
    "lots of data science decisions automatically along the way.\n",
    "\n",
    "Imagine you are adding a new binary classification model to the AutoML repository of algorithms\n",
    "code using Python.\n",
    "\n",
    "Choose one of the following models:\n",
    "- Regularized Logistic Regression (scikit-learn)\n",
    "- Gradient Boosting Machine (any: scikit-learn, XGBoost or LightGBM)\n",
    "- Neural Network (Keras), with the architecture of your choice\n",
    "\n",
    "Write a Python class, using principles of object-oriented design, to wrap the appropriate\n",
    "estimator with the following functionality.\n",
    "\n",
    "**Note** that all preprocessing should be done inside of this class.\n",
    "You can delegate preprocessing to other helper classes if you’d like to, but the user\n",
    "should be able to supply raw, unencoded features to all methods above.  \n",
    "\n",
    "### Dataset\n",
    "It contains a binary classification target for predicting loan defaults: https://drive.google.com/file/d/1rZaXyPd03GJ_xlLP39YGhCD7FdVkhh2o/view?usp=sharing\n",
    "\n",
    "### Unit Tests\n",
    "Please write the unit tests to check whether your model:\n",
    "- is reproducible\n",
    "- can handle missing values\n",
    "- can handle new category levels at prediction time\n",
    "- returns results in the expected format\n",
    "- other useful unit tests you may think of (if time allows)\n",
    "\n",
    "### Notes\n",
    "Assume the data can have numeric and categorical variables. Both your training and prediction functions will take in a two-dimensional pandas DataFrame with a mixture of categorical and numeric variables.\n",
    "\n",
    "Please adhere to [PEP-8](https://www.python.org/dev/peps/pep-0008) guidelines and following standard OOP practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    List,\n",
    "    Set,\n",
    "    Tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    \"\"\"Base class for all estimators\"\"\"\n",
    "    \n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: np.ndarray) -> None:\n",
    "        \"\"\"Fits on training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features\n",
    "        y : np.ndarray\n",
    "            Ground truth labels as a numpy array of 0-s and 1-s.\n",
    " \n",
    " \n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> y = np.array([0, 0, 1])\n",
    "        >>> self.fit(X, y)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predicts class labels on new data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features\n",
    "\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Predicted class labels\n",
    "\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> self.predict(X)\n",
    "        np.array([0, 0, 1])\n",
    "        \"\"\"\n",
    "        return np.array()\n",
    "    \n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Predicts the probability of each label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features\n",
    "\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Predicted probability of each label\n",
    "\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> self.predict_proba(X)\n",
    "        np.array([[0.2, 0.8], [0.9, 0.1], [0.5, 0.5]])\n",
    "        \"\"\"\n",
    "        return np.array()\n",
    "    \n",
    "    def evaluate(self, X: pd.DataFrame, y: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluates \"under the hood\" model.\n",
    "        \n",
    "        Providing features X and Ground truth labels gets \n",
    "        the value of the following metrics: \n",
    "            1. `F1-score <https://en.wikipedia.org/wiki/F1_score>`_\n",
    "            2. `LogLoss <https://en.wikipedia.org/wiki/Loss_functions_for_classification#Logistic_loss>`_\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features\n",
    "        y : np.ndarray\n",
    "            Ground truth labels as a numpy array of 0-s and 1-s.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Predicted probability of each label\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> y = np.array([0, 0, 1])\n",
    "        >>> self.evaluate(X, y)\n",
    "        {'f1_score': 0.3, 'logloss': 0.7}\n",
    "        \"\"\"\n",
    "        return {'f1_score': 0.0, 'logloss': 0.0}\n",
    "\n",
    "    def tune_parameters(self, X: pd.DataFrame, y: np.ndarray) -> Dict[str, Dict]:\n",
    "        \"\"\"Tunes parameters of \"under the hood\" model.\n",
    "        \n",
    "        Finds the best hyperparameters using K-Fold cross-validation for evaluation.\n",
    "        The user is not required to provide a parameter search space. This Estimator \n",
    "        picks a search space on its own.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features\n",
    "        y : np.ndarray\n",
    "            Ground truth labels as a numpy array of 0-s and 1-s.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Output the best parameters and the mean CV score they achieve.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> y = np.array([0, 0, 1])\n",
    "        >>> self.tune_parameters(X, y)\n",
    "        {\n",
    "            'best_parameters': {'C': 1.0, 'fit_intercept': False},\n",
    "            'best_scores': {'f1_score': 0.3, 'logloss': 0.7},\n",
    "        }\n",
    "        \"\"\"\n",
    "        return {'best_parameters': {}, 'best_scores': {'f1_score': 0.0, 'logloss': 0.0}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from dataclasses import dataclass, field, replace\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FeatureStats:\n",
    "    \"\"\"Holds summary stats about feature values\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    index: int\n",
    "    unique_num: int = 0\n",
    "    missed_num: int = 0\n",
    "    \n",
    "    def reindex(self, index):        \n",
    "        return replace(self, index=index)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f'name={self.name}\\n'\n",
    "            f'index={self.index}\\n'\n",
    "            f'unique_num={self.unique_num}\\n'\n",
    "            f'missed_num={self.missed_num}'\n",
    "        )\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class ExploratoryReport:\n",
    "    \"\"\"Holds summary of data set main characteristics\"\"\" \n",
    "\n",
    "    objects_num: int = 0\n",
    "    numeric_features: List[FeatureStats] = field(default_factory=list)\n",
    "    categor_features: List[FeatureStats] = field(default_factory=list)\n",
    "    numeric_features_idx: Dict[str, FeatureStats] = field(init=False, compare=False, repr=False, hash=None, default_factory=dict) \n",
    "    categor_features_idx: Dict[str, FeatureStats] = field(init=False, compare=False, repr=False, hash=None, default_factory=dict)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.numeric_features_idx = {f.name: f for f in self.numeric_features}\n",
    "        self.categor_features_idx = {f.name: f for f in self.categor_features}\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f'objects_num={self.objects_num}\\n'\n",
    "            f'numeric_features={self.numeric_features}\\n'\n",
    "            f'categor_features={self.categor_features}'\n",
    "        )\n",
    "    \n",
    "    \n",
    "class ExploratoryDataAnalyst:\n",
    "    \"\"\"Base class for different approaches to analyzing data sets to summarize \n",
    "       their main characteristics\n",
    "    \"\"\"    \n",
    "    \n",
    "    def analyze(self, X: pd.DataFrame, y: np.ndarray) -> ExploratoryReport:\n",
    "        \"\"\"Analyzes provided dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pd.DataFrame\n",
    "            A collection of objects (objects-features matrix)\n",
    "        y: np.ndarray\n",
    "            Target variables (ground truth labels).\n",
    "            \n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        ExploratoryReport\n",
    "\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> y = np.array([0, 0, 1])\n",
    "        >>> report = ExploratoryDataAnalyst.analyze(X, y)\n",
    "        \"\"\"\n",
    "        \n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "        categor_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
    "        \n",
    "        numeric_features = [\n",
    "            FeatureStats(\n",
    "                name=c,\n",
    "                index=X.columns.get_loc(c),\n",
    "                unique_num=len(c_values.unique()),\n",
    "                missed_num=c_values.isna().sum()\n",
    "            ) for (c, c_values) in ((c, X[c]) for c in numeric_cols)]\n",
    "\n",
    "        categor_features = [\n",
    "            FeatureStats(\n",
    "                name=c,\n",
    "                index=X.columns.get_loc(c),\n",
    "                unique_num=len(c_values.unique()),\n",
    "                missed_num=c_values.isna().sum()\n",
    "            ) for (c, c_values) in ((c, X[c]) for c in categor_cols)]\n",
    "\n",
    "        return ExploratoryReport(\n",
    "            objects_num=len(X), \n",
    "            numeric_features=numeric_features, \n",
    "            categor_features=categor_features\n",
    "        )\n",
    "        \n",
    "\n",
    "class PreprocessingExpert(BaseEstimator, TransformerMixin):  \n",
    "    \"\"\"Plans a strategy of data preprocessing.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 correct_misses_numerics_strategy: str = 'mean', \n",
    "                 correct_misses_category_strategy: str = 'most_frequent', \n",
    "                 transform_feat_numerics_strategy: str = 'standardize', \n",
    "                 transform_feat_category_strategy: str = 'ohe'):       \n",
    "        \"\"\"Creates auto-preprocessing estimator and transformer which makes preprocessing decisions on its own.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        correct_misses_numerics_strategy : one from {'mean', 'median'}, default 'mean'. \n",
    "            If multiple strategies is selected than correct_misses_numerics_columns must be specified.\n",
    "                \n",
    "        correct_misses_category_strategy : list of items from {'most_frequent'}, default ['most_frequent']. \n",
    "            If multiple strategies is selected than correct_misses_category_columns must be specified. \n",
    "        \n",
    "        transform_feat_numerics_strategy : list of items from {'standardize', 'minmax'}, default ['standardize']. \n",
    "            If multiple strategies is selected than transform_feat_numerics_columns must be specified. \n",
    "                \n",
    "        transform_feat_category_strategy : list of items from {'ohe'}, default ['ohe']. \n",
    "            If multiple strategies is selected than transform_feat_numerics_columns must be specified.  \n",
    "        \"\"\"\n",
    "        \n",
    "        # this default parameters map can be changed to reflect input data characteristics after\n",
    "        # fit method is called\n",
    "        self._possible_strategies = {\n",
    "            'correct_misses': {\n",
    "                'numerics_strategy': ['mean', 'median'],\n",
    "                'category_strategy': ['most_frequent']\n",
    "            },\n",
    "            'transform_feat': {\n",
    "                'numerics_strategy': ['standardize', 'minmax'],\n",
    "                'category_strategy': ['ohe']\n",
    "            }\n",
    "        }\n",
    "                \n",
    "        self.correct_misses_numerics_strategy = correct_misses_numerics_strategy if correct_misses_numerics_strategy else 'mean'\n",
    "        self.correct_misses_category_strategy = correct_misses_category_strategy if correct_misses_category_strategy else 'most_frequent'\n",
    "        self.transform_feat_numerics_strategy = transform_feat_numerics_strategy if transform_feat_numerics_strategy else 'standardize'\n",
    "        self.transform_feat_category_strategy = transform_feat_category_strategy if transform_feat_category_strategy else 'ohe'\n",
    "                    \n",
    "            \n",
    "    def fit(self, X, y=None, data_report: ExploratoryReport = None):\n",
    "        if not data_report:\n",
    "            data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "            \n",
    "        self._preprocessing_strategy = self.plan(data_report)\n",
    "        self._preprocessing_strategy.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self._preprocessing_strategy.transform(X)\n",
    "\n",
    "    def get_possible_strategies(self):\n",
    "        return copy.deepcopy(self._possible_strategies)\n",
    "    \n",
    "    def plan(self, data_report: ExploratoryReport) -> Pipeline:\n",
    "        \"\"\"Plans a strategy of dataset preprocessing based on its data report.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_report : ExploratoryReport\n",
    "            Dataset description with different statistics\n",
    "\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        Pipeline\n",
    "            The suggested strategy represented as Pipeline\n",
    "\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> y = np.array([0, 0, 1])\n",
    "        >>> dr = ExploratoryDataAnalyst.analyze(X, y)\n",
    "        >>> self.plan(data_report=dr)\n",
    "        \"\"\"\n",
    "        \n",
    "        correct_misses_strategy, adj_report1 = self.plan_missing_values_strategy(data_report=data_report)\n",
    "        transform_feat_strategy, adj_report2 = self.plan_transform_values_strategy(data_report=adj_report1)\n",
    "    \n",
    "        return Pipeline(steps=[\n",
    "            ('correct_misses', correct_misses_strategy),\n",
    "            ('transform_feat', transform_feat_strategy)\n",
    "        ])\n",
    "        \n",
    "    def plan_missing_values_strategy(self, data_report: ExploratoryReport) -> Tuple[Pipeline, ExploratoryReport]:\n",
    "        \"\"\"This method owns knowledge about how to deal with missing values.\n",
    "\n",
    "        Plans a strategy of dealing with missing values for dataset based on its data report.\n",
    "        It tries to guess what are the possible strategies and what is the best strategy \n",
    "        to apply to such dataset. Different strategies will lead to different output shape,\n",
    "        so planning also includes adjusting the input data report to correctly handle output \n",
    "        data in further preprocessing.\n",
    "\n",
    "        Strategies depend on data statistics, selected model, etc. and can be various:\n",
    "           - remove objects\n",
    "           - remove feature\n",
    "           - remove feature if collinear with other features\n",
    "           - impute mean, moda\n",
    "           - impute using KNN\n",
    "          ...\n",
    "          \n",
    "          \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_report : ExploratoryReport\n",
    "            Dataset description with different statistics\n",
    "            \n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        Tuple[Pipeline, ExploratoryReport]\n",
    "            the first tuple component is the suggested strategy represented as Pipeline\n",
    "            the second tuple component is the adjusted data report for future use\n",
    "        \"\"\"\n",
    "            \n",
    "        numeric_cols = [f.index for f in data_report.numeric_features]\n",
    "        categor_cols = [f.index for f in data_report.categor_features]\n",
    "        \n",
    "        nums_sz = len(numeric_cols)\n",
    "        cats_sz = len(categor_cols)\n",
    "        \n",
    "        num_nan_miss_vals_imputer = SimpleImputer(missing_values=np.nan, strategy=self.correct_misses_numerics_strategy)\n",
    "        cat_nan_miss_vals_imputer = SimpleImputer(missing_values=np.nan, strategy=self.correct_misses_category_strategy)\n",
    "        \n",
    "        adjusted_report = replace(\n",
    "            data_report, \n",
    "            numeric_features=[f.reindex(index=i) \n",
    "                              for i, f in zip(range(nums_sz), data_report.numeric_features)], \n",
    "            categor_features=[f.reindex(index=i) \n",
    "                              for i, f in zip(range(nums_sz, nums_sz+cats_sz), data_report.categor_features)]\n",
    "        )\n",
    "        \n",
    "        ct = ColumnTransformer([\n",
    "            # these transformers do not change the input shape\n",
    "            # TODO: open question is what to do if the input shape changed and we\n",
    "            #       can not rely on data_report in next transformers\n",
    "            ('num_nan_miss_vals_imputer', num_nan_miss_vals_imputer, numeric_cols),\n",
    "            ('cat_nan_miss_vals_imputer', cat_nan_miss_vals_imputer, categor_cols)\n",
    "        ])\n",
    "        \n",
    "        return ct, adjusted_report\n",
    " \n",
    "    def plan_transform_values_strategy(self, data_report: ExploratoryReport) -> Tuple[Pipeline, ExploratoryReport]:\n",
    "        \"\"\"This method owns knowledge about how to transform different features.\n",
    "\n",
    "        Strategies depend on data statistics and can be various for different feature\n",
    "        scales - absolute aka numeric, interval, ordinal, nominal aka categorical - and \n",
    "        admissible set of operations defined for each scale.\n",
    "\n",
    "        Upproaches can vary depending on selected model, data sparsity, number of outliers, \n",
    "        correlation and others.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        report : ExploratoryReport\n",
    "            Dataset description with different statistics\n",
    "            \n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        Tuple[Pipeline, ExploratoryReport]\n",
    "            the first tuple component is the suggested strategy represented as Pipeline\n",
    "            the second tuple component is the adjusted data report for future use\n",
    "        \"\"\"\n",
    "    \n",
    "        numeric_cols = [f.index for f in data_report.numeric_features]\n",
    "        categor_cols = [f.index for f in data_report.categor_features]\n",
    "        \n",
    "        if self.transform_feat_numerics_strategy == 'standardize':\n",
    "            numerics_step = ('num_transform_standardize', StandardScaler(), numeric_cols)\n",
    "        else:\n",
    "            numerics_step = ('num_transform_minmax', MinMaxScaler(), numeric_cols)\n",
    "            \n",
    "        ct = ColumnTransformer([\n",
    "            numerics_step,\n",
    "            \n",
    "            # these transformer will change the input shape, so futher\n",
    "            # one can not use categor_cols from data_report\n",
    "            ('cat_transform_oneh_encode', OneHotEncoder(sparse=False, handle_unknown='ignore'), categor_cols)\n",
    "        ])\n",
    "\n",
    "        return ct, None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "    \n",
    "class LogRegClassifier(Estimator):\n",
    "    \"\"\"An automated version of Regularized Logistic Regression based on \n",
    "       scikit-learn SGDClassifier classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = None):\n",
    "        \"\"\"Basic constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int, default=None\n",
    "            Random seed to force reproducibility while testing\n",
    "\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> cls = LogRegClassifier(random_seed=242)\n",
    "        >>> X = pd.DataFrame({'feat1': ['a', 'b', 'a'], 'feat2': [1, 2, 3]})\n",
    "        >>> y = np.array([0, 0, 1])\n",
    "        >>> cls.fit(X,y)\n",
    "        \"\"\"\n",
    "        self._clf_pipeline = None\n",
    "        self._expected_objects_features = None\n",
    "        self._rseed = random_state\n",
    "        self._epsilon = 1e-14\n",
    "\n",
    "    def _assert_objects_features(self, X: pd.DataFrame):\n",
    "        if list(X.columns) != self._expected_objects_features:\n",
    "            raise Exception('Input features do not match expected list ' + self._expected_objects_features)\n",
    "            \n",
    "    def _assert_is_binary_target(self, y: np.ndarray):\n",
    "        if len(np.unique(y)) != 2:\n",
    "            raise Exception('Only Binary classification is supported')\n",
    "            \n",
    "    def _create_base_model(self):       \n",
    "#         return SGDClassifier(\n",
    "#             loss='log', \n",
    "#             penalty='l2', \n",
    "#             fit_intercept=False, \n",
    "#             max_iter=1000, \n",
    "#             tol=1e-4,\n",
    "#             epsilon=self._epsilon,\n",
    "#             shuffle=True,\n",
    "#             random_state=self._rseed,\n",
    "#             learning_rate='optimal'\n",
    "#         )\n",
    "        return LogisticRegression(\n",
    "            penalty='l2',\n",
    "            dual=False,\n",
    "            tol=1e-4,\n",
    "            C=1.0,\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            n_jobs=-1,\n",
    "            random_state=self._rseed\n",
    "        )\n",
    "\n",
    "    def _create_default_pipeline(self):  \n",
    "        return Pipeline(steps=[\n",
    "            ('preprocessings', PreprocessingExpert()),\n",
    "            ('logit_classify', self._create_base_model())\n",
    "        ])\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: np.ndarray) -> None:\n",
    "        self._assert_is_binary_target(y)\n",
    "        \n",
    "        self._expected_objects_features = list(X.columns)\n",
    "       \n",
    "        data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "        self._clf_pipeline = self._create_default_pipeline()\n",
    "        \n",
    "        self._clf_pipeline.fit(X, y, preprocessings__data_report=data_report)\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        self._assert_objects_features(X)\n",
    "        return self._clf_pipeline.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        self._assert_objects_features(X)\n",
    "        return self._clf_pipeline.predict_proba(X)\n",
    "    \n",
    "    def evaluate(self, X: pd.DataFrame, y_true: np.ndarray) -> Dict[str, float]:\n",
    "        self._assert_objects_features(X)\n",
    "        self._assert_is_binary_target(y_true)\n",
    "        \n",
    "        y_pred = self._clf_pipeline.predict(X)\n",
    "        y_pred_proba = self._clf_pipeline.predict_proba(X)\n",
    "        \n",
    "        return {\n",
    "            'f1_score': f1_score(y_true=y_true, y_pred=y_pred, average='binary'), \n",
    "            'logloss': log_loss(y_true=y_true, y_pred=y_pred_proba, eps=self._epsilon)\n",
    "        }\n",
    "    \n",
    "    def tune_parameters(self, X: pd.DataFrame, y: np.ndarray) -> Dict[str, Dict]:\n",
    "        self._assert_is_binary_target(y)\n",
    "        \n",
    "        self._expected_objects_features = list(X.columns)\n",
    "        \n",
    "        data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "        preprocessings = PreprocessingExpert().fit(X, y, data_report=data_report)\n",
    "        possible_strategies = preprocessings.get_possible_strategies()\n",
    "        \n",
    "        dict_merge = lambda a,b: a.update(b) or a\n",
    "                    \n",
    "        preprocessings_params = {\n",
    "            'preprocessings__correct_misses_numerics_strategy': possible_strategies['correct_misses']['numerics_strategy'],\n",
    "            'preprocessings__correct_misses_category_strategy': possible_strategies['correct_misses']['category_strategy'],\n",
    "            'preprocessings__transform_feat_numerics_strategy': possible_strategies['transform_feat']['numerics_strategy'],\n",
    "            'preprocessings__transform_feat_category_strategy': possible_strategies['transform_feat']['category_strategy']\n",
    "        }\n",
    "            \n",
    "        base_params = dict_merge(preprocessings_params, {\n",
    "            #'logit_classify__dual': [True, False],\n",
    "            'logit_classify__C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 5, 10, 50, 100, 1000]\n",
    "        })\n",
    "        \n",
    "        param_grid = [\n",
    "            dict_merge(base_params, {\n",
    "                #'logit_classify__solver': ['liblinear', 'saga'],\n",
    "                #'logit_classify__penalty': ['l1', 'l2']\n",
    "            }), \n",
    "#             dict_merge(base_params, {\n",
    "#                 #'logit_classify__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "#                 #'logit_classify__penalty': ['l2']\n",
    "#             })\n",
    "        ]\n",
    "        \n",
    "        print(param_grid)\n",
    "        \n",
    "        # n_splits should be adaptive\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=self._rseed)\n",
    "        \n",
    "        scoring = {\n",
    "            'AUC': 'roc_auc', \n",
    "            'f1': make_scorer(f1_score, needs_proba=False, average='binary'),\n",
    "            'logloss': make_scorer(log_loss, needs_proba=True, eps=self._epsilon)\n",
    "        }\n",
    "        \n",
    "        self._clf_pipeline = self._create_default_pipeline()\n",
    "        \n",
    "        gs = GridSearchCV(\n",
    "            self._clf_pipeline, \n",
    "            param_grid, \n",
    "            cv=kf, \n",
    "            n_jobs=-1, \n",
    "            verbose=1, \n",
    "            scoring=scoring,\n",
    "            refit='AUC', \n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        gs.fit(X, y)\n",
    "        self._clf_pipeline = gs.best_estimator_\n",
    "        \n",
    "        return {\n",
    "            'best_parameters': gs.best_params_, \n",
    "            'best_scores': {\n",
    "                'f1_score': gs.cv_results_['mean_test_f1'][gs.best_index_], \n",
    "                'logloss': gs.cv_results_['mean_test_logloss'][gs.best_index_]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "from unittest.mock import patch\n",
    "from numpy.testing import assert_array_equal\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "__file__ = 'experiments.ipynb'\n",
    "\n",
    "\n",
    "lending_club = pd.read_csv(pathlib.Path().cwd() / 'Lending_Club_reduced.csv')\n",
    "lending_club_y = lending_club['is_bad'].to_numpy()\n",
    "lending_club_x = lending_club.drop(['is_bad'], axis=1)\n",
    "del lending_club\n",
    "\n",
    "\n",
    "def assert_raise_message(exception, message, function, *args, **kwargs):\n",
    "    try:\n",
    "        function(*args, **kwargs)\n",
    "    except exceptions as e:\n",
    "        error_message = str(e)\n",
    "        if message != error_message:\n",
    "            raise AssertionError(f\"Error message does equal to the expected string: {message}. Observed error message: {error_message}\")\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def exploratory_test_report():\n",
    "    numeric_features = [\n",
    "        FeatureStats(\n",
    "            name='num_test'+str(i),\n",
    "            index=i,\n",
    "            unique_num=42*i,\n",
    "            missed_num=66*i) for i in range(2)]\n",
    " \n",
    "    categor_features = [\n",
    "        FeatureStats(\n",
    "            name='cat_test'+str(i),\n",
    "            index=len(numeric_features)+i,\n",
    "            unique_num=42*(len(numeric_features)+i),\n",
    "            missed_num=66*(len(numeric_features)+i)) for i in range(2)]\n",
    "\n",
    "    return ExploratoryReport(\n",
    "        objects_num=11, \n",
    "        numeric_features=numeric_features, \n",
    "        categor_features=categor_features\n",
    "      )\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def lending_club_data():\n",
    "    return (lending_club_x, lending_club_y)\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def lending_club_numeric_features():\n",
    "    return [\n",
    "        FeatureStats(name='Id', index=0, unique_num=10000, missed_num=0), \n",
    "        FeatureStats(name='annual_inc', index=3, unique_num=1902, missed_num=1), \n",
    "        FeatureStats(name='debt_to_income', index=9, unique_num=2585, missed_num=0), \n",
    "        FeatureStats(name='delinq_2yrs', index=10, unique_num=11, missed_num=5), \n",
    "        FeatureStats(name='inq_last_6mths', index=11, unique_num=21, missed_num=5), \n",
    "        FeatureStats(name='mths_since_last_delinq', index=12, unique_num=92, missed_num=6316), \n",
    "        FeatureStats(name='mths_since_last_record', index=13, unique_num=95, missed_num=9160), \n",
    "        FeatureStats(name='open_acc', index=14, unique_num=37, missed_num=5), \n",
    "        FeatureStats(name='pub_rec', index=15, unique_num=5, missed_num=5), \n",
    "        FeatureStats(name='revol_bal', index=16, unique_num=8130, missed_num=0), \n",
    "        FeatureStats(name='revol_util', index=17, unique_num=1028, missed_num=26), \n",
    "        FeatureStats(name='total_acc', index=18, unique_num=76, missed_num=5), \n",
    "        FeatureStats(name='collections_12_mths_ex_med', index=20, unique_num=2, missed_num=32), \n",
    "        FeatureStats(name='mths_since_last_major_derog', index=21, unique_num=3, missed_num=0)\n",
    "    ]\n",
    "    \n",
    "\n",
    "@pytest.fixture\n",
    "def lending_club_categor_features():\n",
    "    return [\n",
    "        FeatureStats(name='emp_length', index=1, unique_num=14, missed_num=0), \n",
    "        FeatureStats(name='home_ownership', index=2, unique_num=5, missed_num=0), \n",
    "        FeatureStats(name='verification_status', index=4, unique_num=3, missed_num=0), \n",
    "        FeatureStats(name='pymnt_plan', index=5, unique_num=2, missed_num=0), \n",
    "        FeatureStats(name='purpose_cat', index=6, unique_num=27, missed_num=0), \n",
    "        FeatureStats(name='zip_code', index=7, unique_num=720, missed_num=0), \n",
    "        FeatureStats(name='addr_state', index=8, unique_num=50, missed_num=0), \n",
    "        FeatureStats(name='initial_list_status', index=19, unique_num=2, missed_num=0), \n",
    "        FeatureStats(name='policy_code', index=22, unique_num=5, missed_num=0)\n",
    "    ]\n",
    "    \n",
    "\n",
    "@pytest.fixture\n",
    "def adjusted_data_report_after_correct_misses_step():\n",
    "    return ExploratoryReport(\n",
    "        objects_num=10000, \n",
    "        numeric_features=[FeatureStats(name='Id', index=0, unique_num=10000, missed_num=0), \n",
    "                          FeatureStats(name='annual_inc', index=1, unique_num=1902, missed_num=1), \n",
    "                          FeatureStats(name='debt_to_income', index=2, unique_num=2585, missed_num=0), \n",
    "                          FeatureStats(name='delinq_2yrs', index=3, unique_num=11, missed_num=5), \n",
    "                          FeatureStats(name='inq_last_6mths', index=4, unique_num=21, missed_num=5), \n",
    "                          FeatureStats(name='mths_since_last_delinq', index=5, unique_num=92, missed_num=6316), \n",
    "                          FeatureStats(name='mths_since_last_record', index=6, unique_num=95, missed_num=9160), \n",
    "                          FeatureStats(name='open_acc', index=7, unique_num=37, missed_num=5), \n",
    "                          FeatureStats(name='pub_rec', index=8, unique_num=5, missed_num=5), \n",
    "                          FeatureStats(name='revol_bal', index=9, unique_num=8130, missed_num=0), \n",
    "                          FeatureStats(name='revol_util', index=10, unique_num=1028, missed_num=26), \n",
    "                          FeatureStats(name='total_acc', index=11, unique_num=76, missed_num=5), \n",
    "                          FeatureStats(name='collections_12_mths_ex_med', index=12, unique_num=2, missed_num=32), \n",
    "                          FeatureStats(name='mths_since_last_major_derog', index=13, unique_num=3, missed_num=0)], \n",
    "        categor_features=[FeatureStats(name='emp_length', index=14, unique_num=14, missed_num=0), \n",
    "                          FeatureStats(name='home_ownership', index=15, unique_num=5, missed_num=0), \n",
    "                          FeatureStats(name='verification_status', index=16, unique_num=3, missed_num=0), \n",
    "                          FeatureStats(name='pymnt_plan', index=17, unique_num=2, missed_num=0), \n",
    "                          FeatureStats(name='purpose_cat', index=18, unique_num=27, missed_num=0), \n",
    "                          FeatureStats(name='zip_code', index=19, unique_num=720, missed_num=0), \n",
    "                          FeatureStats(name='addr_state', index=20, unique_num=50, missed_num=0), \n",
    "                          FeatureStats(name='initial_list_status', index=21, unique_num=2, missed_num=0), \n",
    "                          FeatureStats(name='policy_code', index=22, unique_num=5, missed_num=0)]\n",
    "    )\n",
    "\n",
    "\n",
    "preprocessing_expert_cm_numerics_strategy_params = pytest.mark.parametrize('cm_numerics_strategy', [None, 'mean', 'median'])\n",
    "preprocessing_expert_cm_category_strategy_params = pytest.mark.parametrize('cm_category_strategy', [None, 'most_frequent'])\n",
    "preprocessing_expert_tf_numerics_strategy_params = pytest.mark.parametrize('tf_numerics_strategy', [None, 'standardize', 'minmax'])\n",
    "preprocessing_expert_tf_category_strategy_params = pytest.mark.parametrize('tf_category_strategy', [None, 'ohe'])\n",
    "\n",
    "preprocessing_expert_cm_numerics_strategy_params_no_none = pytest.mark.parametrize('cm_numerics_strategy', ['mean', 'median'])\n",
    "preprocessing_expert_cm_category_strategy_params_no_none = pytest.mark.parametrize('cm_category_strategy', ['most_frequent'])\n",
    "preprocessing_expert_tf_numerics_strategy_params_no_none = pytest.mark.parametrize('tf_numerics_strategy', ['standardize', 'minmax'])\n",
    "preprocessing_expert_tf_category_strategy_params_no_none = pytest.mark.parametrize('tf_category_strategy', ['ohe'])\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def default_preprocessing_expert_strategies():\n",
    "    return {\n",
    "        'correct_misses': {\n",
    "            'numerics_strategy': ['mean', 'median'],\n",
    "            'category_strategy': ['most_frequent']\n",
    "        },\n",
    "        'transform_feat': {\n",
    "            'numerics_strategy': ['standardize', 'minmax'],\n",
    "            'category_strategy': ['ohe']\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def random_state():\n",
    "    return 242\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_report = ExploratoryDataAnalyst().analyze(lending_club_x, lending_club_y)\n",
    "\n",
    "\n",
    "# pe = PreprocessingExpert(\n",
    " \n",
    "# ) \n",
    "\n",
    "# pipeline = pe.plan(data_report=data_report)\n",
    "\n",
    "lending_club_x\n",
    "\n",
    "# print(repr(pipeline.get_params()))\n",
    "for cm_numerics_strategy in ['mean', 'median']:\n",
    "    for cm_category_strategy in ['most_frequent']:\n",
    "        for tf_numerics_strategy in ['standardize', 'minmax']:\n",
    "            for tf_category_strategy in ['ohe']:\n",
    "            \n",
    "                fn = [f.name for f in lending_club_numeric_features()]\n",
    "                fc = [f.name for f in lending_club_categor_features()]\n",
    "\n",
    "                t = SimpleImputer(missing_values=np.nan, strategy=cm_numerics_strategy)\n",
    "                r = t.fit_transform(lending_club_x[fn], lending_club_y)\n",
    "\n",
    "                t2 = SimpleImputer(missing_values=np.nan, strategy=cm_category_strategy)\n",
    "                r2 = t2.fit_transform(lending_club_x[fc], lending_club_y)\n",
    "\n",
    "                t3 = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "                r3 = t3.fit_transform(r2, y)\n",
    "                \n",
    "                if tf_numerics_strategy == 'standardize':\n",
    "                    t4 = StandardScaler()\n",
    "                else:\n",
    "                    t4 = MinMaxScaler()\n",
    "                    \n",
    "                r4 = t4.fit_transform(r, y)\n",
    "                \n",
    "                np.savez_compressed(f'cm__{cm_numerics_strategy}-{cm_category_strategy}__ft__{tf_numerics_strategy}-{tf_category_strategy}.npz', np.hstack((r4, r3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_merge = lambda a,b: a.update(b) or a\n",
    "\n",
    "\n",
    "def test_feature_stats():\n",
    "    fs = FeatureStats(\n",
    "            name='test',\n",
    "            index=21,\n",
    "            unique_num=42,\n",
    "            missed_num=66\n",
    "         )\n",
    "    \n",
    "    assert fs.name == 'test'\n",
    "    assert fs.index == 21\n",
    "    assert fs.unique_num == 42\n",
    "    assert fs.missed_num == 66\n",
    "    assert str(fs) == 'name=test\\nindex=21\\nunique_num=42\\nmissed_num=66'\n",
    "            \n",
    "\n",
    "def test_feature_stats_reindex():\n",
    "    fs = FeatureStats(\n",
    "            name='test',\n",
    "            index=21,\n",
    "            unique_num=42,\n",
    "            missed_num=66\n",
    "         )\n",
    "\n",
    "    fs2 = fs.reindex(index=33)\n",
    "    \n",
    "    assert fs != fs2\n",
    "    assert fs2.index == 33\n",
    "    \n",
    "\n",
    "def test_exploratory_report(exploratory_test_report):\n",
    "    er = exploratory_test_report\n",
    "    \n",
    "    numeric_features = [\n",
    "        FeatureStats(name='num_test0',index=0,unique_num=0,missed_num=0), \n",
    "        FeatureStats(name='num_test1',index=1,unique_num=42,missed_num=66)\n",
    "    ]\n",
    "    \n",
    "    categor_features = [\n",
    "        FeatureStats(name='cat_test0',index=2,unique_num=84,missed_num=132), \n",
    "        FeatureStats(name='cat_test1',index=3,unique_num=126,missed_num=198)\n",
    "    ]\n",
    "    \n",
    "    numeric_features_idx = {f.name: f for f in numeric_features}\n",
    "    categor_features_idx = {f.name: f for f in categor_features}\n",
    "        \n",
    "    assert er.objects_num == 11\n",
    "    assert er.numeric_features == numeric_features\n",
    "    assert er.categor_features == categor_features\n",
    "    assert er.numeric_features_idx == numeric_features_idx\n",
    "    assert er.categor_features_idx == categor_features_idx\n",
    "    assert str(er) == \"objects_num=11\\nnumeric_features=[FeatureStats(name='num_test0', index=0, unique_num=0, missed_num=0), FeatureStats(name='num_test1', index=1, unique_num=42, missed_num=66)]\\ncategor_features=[FeatureStats(name='cat_test0', index=2, unique_num=84, missed_num=132), FeatureStats(name='cat_test1', index=3, unique_num=126, missed_num=198)]\"\n",
    "    \n",
    "    \n",
    "def test_exploratory_data_analist(lending_club_data, lending_club_numeric_features, lending_club_categor_features):\n",
    "    X, y = lending_club_data\n",
    "    data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "            \n",
    "    assert data_report.objects_num == 10000\n",
    "    assert data_report.numeric_features == lending_club_numeric_features\n",
    "    assert data_report.categor_features == lending_club_categor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@preprocessing_expert_tf_numerics_strategy_params\n",
    "@preprocessing_expert_tf_category_strategy_params\n",
    "def test_preprocessing_expert_plan_transform_values_strategy(lending_club_data,\n",
    "                                                             tf_numerics_strategy, \n",
    "                                                             tf_category_strategy):\n",
    "    X, y = lending_club_data\n",
    "    data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "\n",
    "    pe = PreprocessingExpert(transform_feat_numerics_strategy=tf_numerics_strategy, \n",
    "                             transform_feat_category_strategy=tf_category_strategy) \n",
    "\n",
    "    pipeline, dr = pe.plan_transform_values_strategy(data_report=data_report)\n",
    "    \n",
    "    expected_parameters = {\n",
    "        'None_None'  : (\"{'n_jobs': None, 'remainder': 'drop', 'sparse_threshold': 0.3, 'transformer_weights': None, 'transformers': [('num_transform_standardize', StandardScaler(copy=True, with_mean=True, with_std=True), [0, 3, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21]), ('cat_transform_oneh_encode', OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\\n\"\n",
    "                        \"              handle_unknown='ignore', sparse=False), [1, 2, 4, 5, 6, 7, 8, 19, 22])], 'verbose': False, 'num_transform_standardize': StandardScaler(copy=True, with_mean=True, with_std=True), 'cat_transform_oneh_encode': OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\\n\"\n",
    "                        \"              handle_unknown='ignore', sparse=False), 'num_transform_standardize__copy': True, 'num_transform_standardize__with_mean': True, 'num_transform_standardize__with_std': True, 'cat_transform_oneh_encode__categories': 'auto', 'cat_transform_oneh_encode__drop': None, 'cat_transform_oneh_encode__dtype': <class 'numpy.float64'>, 'cat_transform_oneh_encode__handle_unknown': 'ignore', 'cat_transform_oneh_encode__sparse': False}\"),        \n",
    "        'minmax_None': (\"{'n_jobs': None, 'remainder': 'drop', 'sparse_threshold': 0.3, 'transformer_weights': None, 'transformers': [('num_transform_minmax', MinMaxScaler(copy=True, feature_range=(0, 1)), [0, 3, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21]), ('cat_transform_oneh_encode', OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\\n\"\n",
    "                        \"              handle_unknown='ignore', sparse=False), [1, 2, 4, 5, 6, 7, 8, 19, 22])], 'verbose': False, 'num_transform_minmax': MinMaxScaler(copy=True, feature_range=(0, 1)), 'cat_transform_oneh_encode': OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\\n\"\n",
    "                        \"              handle_unknown='ignore', sparse=False), 'num_transform_minmax__copy': True, 'num_transform_minmax__feature_range': (0, 1), 'cat_transform_oneh_encode__categories': 'auto', 'cat_transform_oneh_encode__drop': None, 'cat_transform_oneh_encode__dtype': <class 'numpy.float64'>, 'cat_transform_oneh_encode__handle_unknown': 'ignore', 'cat_transform_oneh_encode__sparse': False}\")\n",
    "    }\n",
    "    \n",
    "    expected_parameters = dict_merge(expected_parameters, {\n",
    "        'None_ohe': expected_parameters['None_None'],\n",
    "        'standardize_None': expected_parameters['None_None'],\n",
    "        'standardize_ohe': expected_parameters['None_None'],\n",
    "        'minmax_ohe': expected_parameters['minmax_None']\n",
    "    })\n",
    "    \n",
    "    params_combi = str(tf_numerics_strategy) + '_' + str(tf_category_strategy)\n",
    "    \n",
    "    assert type(pipeline) == ColumnTransformer\n",
    "    assert repr(pipeline.get_params()) == expected_parameters[params_combi]\n",
    "    assert dr is None\n",
    "\n",
    "    \n",
    "@preprocessing_expert_cm_numerics_strategy_params\n",
    "@preprocessing_expert_cm_category_strategy_params\n",
    "def test_preprocessing_expert_plan_missing_values_strategy(lending_club_data,\n",
    "                                                           adjusted_data_report_after_correct_misses_step,\n",
    "                                                           cm_numerics_strategy, \n",
    "                                                           cm_category_strategy):\n",
    "    X, y = lending_club_data\n",
    "    data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "\n",
    "    pe = PreprocessingExpert(correct_misses_numerics_strategy=cm_numerics_strategy, \n",
    "                             correct_misses_category_strategy=cm_category_strategy) \n",
    "\n",
    "    pipeline, dr = pe.plan_missing_values_strategy(data_report=data_report)\n",
    "    \n",
    "    expected_parameters = {\n",
    "        'None_None'  : (\"{'n_jobs': None, 'remainder': 'drop', 'sparse_threshold': 0.3, 'transformer_weights': None, 'transformers': [('num_nan_miss_vals_imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='mean', verbose=0), [0, 3, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21]), ('cat_nan_miss_vals_imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='most_frequent', verbose=0), [1, 2, 4, 5, 6, 7, 8, 19, 22])], 'verbose': False, 'num_nan_miss_vals_imputer': SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='mean', verbose=0), 'cat_nan_miss_vals_imputer': SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='most_frequent', verbose=0), 'num_nan_miss_vals_imputer__add_indicator': False, 'num_nan_miss_vals_imputer__copy': True, 'num_nan_miss_vals_imputer__fill_value': None, 'num_nan_miss_vals_imputer__missing_values': nan, 'num_nan_miss_vals_imputer__strategy': 'mean', 'num_nan_miss_vals_imputer__verbose': 0, 'cat_nan_miss_vals_imputer__add_indicator': False, 'cat_nan_miss_vals_imputer__copy': True, 'cat_nan_miss_vals_imputer__fill_value': None, 'cat_nan_miss_vals_imputer__missing_values': nan, 'cat_nan_miss_vals_imputer__strategy': 'most_frequent', 'cat_nan_miss_vals_imputer__verbose': 0}\"),        \n",
    "        'median_None': (\"{'n_jobs': None, 'remainder': 'drop', 'sparse_threshold': 0.3, 'transformer_weights': None, 'transformers': [('num_nan_miss_vals_imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='median', verbose=0), [0, 3, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21]), ('cat_nan_miss_vals_imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='most_frequent', verbose=0), [1, 2, 4, 5, 6, 7, 8, 19, 22])], 'verbose': False, 'num_nan_miss_vals_imputer': SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='median', verbose=0), 'cat_nan_miss_vals_imputer': SimpleImputer(add_indicator=False, copy=True, fill_value=None,\\n\"\n",
    "                        \"              missing_values=nan, strategy='most_frequent', verbose=0), 'num_nan_miss_vals_imputer__add_indicator': False, 'num_nan_miss_vals_imputer__copy': True, 'num_nan_miss_vals_imputer__fill_value': None, 'num_nan_miss_vals_imputer__missing_values': nan, 'num_nan_miss_vals_imputer__strategy': 'median', 'num_nan_miss_vals_imputer__verbose': 0, 'cat_nan_miss_vals_imputer__add_indicator': False, 'cat_nan_miss_vals_imputer__copy': True, 'cat_nan_miss_vals_imputer__fill_value': None, 'cat_nan_miss_vals_imputer__missing_values': nan, 'cat_nan_miss_vals_imputer__strategy': 'most_frequent', 'cat_nan_miss_vals_imputer__verbose': 0}\")\n",
    "    }\n",
    "    \n",
    "    expected_parameters = dict_merge(expected_parameters, {\n",
    "        'mean_None': expected_parameters['None_None'],\n",
    "        'None_most_frequent': expected_parameters['None_None'],\n",
    "        'mean_most_frequent': expected_parameters['None_None'],\n",
    "        'median_most_frequent': expected_parameters['median_None']\n",
    "    })\n",
    "        \n",
    "    assert type(pipeline) == ColumnTransformer\n",
    "    assert repr(pipeline.get_params()) == expected_parameters[str(cm_numerics_strategy) + '_' + str(cm_category_strategy)]\n",
    "    assert dr is not None\n",
    "    assert dr != data_report\n",
    "    assert dr == adjusted_data_report_after_correct_misses_step\n",
    "\n",
    "\n",
    "# TODO probably unsafe test, how to do it better?\n",
    "@patch.object(PreprocessingExpert, 'plan_transform_values_strategy')\n",
    "@patch.object(PreprocessingExpert, 'plan_missing_values_strategy')\n",
    "@preprocessing_expert_cm_numerics_strategy_params\n",
    "@preprocessing_expert_cm_category_strategy_params\n",
    "@preprocessing_expert_tf_numerics_strategy_params\n",
    "@preprocessing_expert_tf_category_strategy_params\n",
    "def test_preprocessing_expert_plan(plan_transform_values_strategy_mock,\n",
    "                                   plan_missing_values_strategy_mock,\n",
    "                                   lending_club_data,\n",
    "                                   cm_numerics_strategy, \n",
    "                                   cm_category_strategy, \n",
    "                                   tf_numerics_strategy, \n",
    "                                   tf_category_strategy):\n",
    "    X, y = lending_club_data\n",
    "    data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "\n",
    "    plan_transform_values_strategy_mock.return_value = FunctionTransformer(), data_report\n",
    "    plan_missing_values_strategy_mock.return_value = FunctionTransformer(), data_report\n",
    "    \n",
    "    pe = PreprocessingExpert(correct_misses_numerics_strategy=cm_numerics_strategy, \n",
    "                             correct_misses_category_strategy=cm_category_strategy, \n",
    "                             transform_feat_numerics_strategy=tf_numerics_strategy, \n",
    "                             transform_feat_category_strategy=tf_category_strategy) \n",
    "    \n",
    "    pipeline = pe.plan(data_report=data_report)\n",
    "    \n",
    "    plan_missing_values_strategy_mock.assert_called_with(data_report=data_report)\n",
    "    plan_transform_values_strategy_mock.assert_called_with(data_report=data_report)\n",
    "    \n",
    "    assert type(pipeline) == Pipeline\n",
    "    assert repr(pipeline.get_params()) == (\"{'memory': None, 'steps': [('correct_misses', FunctionTransformer(accept_sparse=False, check_inverse=True, func=None,\\n\"\n",
    "                                           \"                    inv_kw_args=None, inverse_func=None, kw_args=None,\\n\"\n",
    "                                           \"                    validate=False)), ('transform_feat', FunctionTransformer(accept_sparse=False, check_inverse=True, func=None,\\n\"\n",
    "                                           \"                    inv_kw_args=None, inverse_func=None, kw_args=None,\\n\"\n",
    "                                           \"                    validate=False))], 'verbose': False, 'correct_misses': FunctionTransformer(accept_sparse=False, check_inverse=True, func=None,\\n\"\n",
    "                                           \"                    inv_kw_args=None, inverse_func=None, kw_args=None,\\n\"\n",
    "                                           \"                    validate=False), 'transform_feat': FunctionTransformer(accept_sparse=False, check_inverse=True, func=None,\\n\"\n",
    "                                           \"                    inv_kw_args=None, inverse_func=None, kw_args=None,\\n\"\n",
    "                                           \"                    validate=False), 'correct_misses__accept_sparse': False, 'correct_misses__check_inverse': True, 'correct_misses__func': None, 'correct_misses__inv_kw_args': None, 'correct_misses__inverse_func': None, 'correct_misses__kw_args': None, 'correct_misses__validate': False, 'transform_feat__accept_sparse': False, 'transform_feat__check_inverse': True, 'transform_feat__func': None, 'transform_feat__inv_kw_args': None, 'transform_feat__inverse_func': None, 'transform_feat__kw_args': None, 'transform_feat__validate': False}\")\n",
    "\n",
    "    \n",
    "@preprocessing_expert_cm_numerics_strategy_params\n",
    "@preprocessing_expert_cm_category_strategy_params\n",
    "@preprocessing_expert_tf_numerics_strategy_params\n",
    "@preprocessing_expert_tf_category_strategy_params\n",
    "def test_preprocessing_expert_get_possible_strategies_without_fit(default_preprocessing_expert_strategies,\n",
    "                                                                  cm_numerics_strategy, \n",
    "                                                                  cm_category_strategy, \n",
    "                                                                  tf_numerics_strategy, \n",
    "                                                                  tf_category_strategy):    \n",
    "    pe = PreprocessingExpert(correct_misses_numerics_strategy=cm_numerics_strategy, \n",
    "                             correct_misses_category_strategy=cm_category_strategy, \n",
    "                             transform_feat_numerics_strategy=tf_numerics_strategy, \n",
    "                             transform_feat_category_strategy=tf_category_strategy) \n",
    "    \n",
    "    possible_strategies1 = pe.get_possible_strategies()\n",
    "    assert possible_strategies1 == default_preprocessing_expert_strategies \n",
    "    \n",
    "\n",
    "@preprocessing_expert_cm_numerics_strategy_params\n",
    "@preprocessing_expert_cm_category_strategy_params\n",
    "@preprocessing_expert_tf_numerics_strategy_params\n",
    "@preprocessing_expert_tf_category_strategy_params\n",
    "def test_preprocessing_expert_get_possible_strategies_immutable(default_preprocessing_expert_strategies,\n",
    "                                                                cm_numerics_strategy, \n",
    "                                                                cm_category_strategy, \n",
    "                                                                tf_numerics_strategy, \n",
    "                                                                tf_category_strategy):    \n",
    "    pe = PreprocessingExpert(correct_misses_numerics_strategy=cm_numerics_strategy, \n",
    "                             correct_misses_category_strategy=cm_category_strategy, \n",
    "                             transform_feat_numerics_strategy=tf_numerics_strategy, \n",
    "                             transform_feat_category_strategy=tf_category_strategy) \n",
    "    \n",
    "    possible_strategies1 = pe.get_possible_strategies()\n",
    "    possible_strategies2 = pe.get_possible_strategies()\n",
    "    \n",
    "    assert possible_strategies1 == default_preprocessing_expert_strategies\n",
    "    assert possible_strategies2 == default_preprocessing_expert_strategies\n",
    "    assert possible_strategies1 is not possible_strategies2\n",
    "    assert possible_strategies1 == possible_strategies2\n",
    "            \n",
    "    possible_strategies1['correct_misses']['numerics_strategy'].append('test')\n",
    "    assert possible_strategies1 != possible_strategies2\n",
    "    possible_strategies1['correct_misses']['numerics_strategy'].remove('test')\n",
    "    assert possible_strategies1 == possible_strategies2\n",
    "\n",
    "    possible_strategies1['correct_misses']['category_strategy'].append('test')\n",
    "    assert possible_strategies1 != possible_strategies2\n",
    "    possible_strategies1['correct_misses']['category_strategy'].remove('test')\n",
    "    assert possible_strategies1 == possible_strategies2\n",
    "    \n",
    "    possible_strategies1['transform_feat']['numerics_strategy'].append('test')\n",
    "    assert possible_strategies1 != possible_strategies2\n",
    "    possible_strategies1['transform_feat']['numerics_strategy'].remove('test')\n",
    "    assert possible_strategies1 == possible_strategies2\n",
    "    \n",
    "    possible_strategies1['transform_feat']['category_strategy'].append('test')\n",
    "    assert possible_strategies1 != possible_strategies2\n",
    "    possible_strategies1['transform_feat']['category_strategy'].remove('test')\n",
    "    assert possible_strategies1 == possible_strategies2\n",
    "    \n",
    "    \n",
    "@preprocessing_expert_cm_numerics_strategy_params\n",
    "@preprocessing_expert_cm_category_strategy_params\n",
    "@preprocessing_expert_tf_numerics_strategy_params\n",
    "@preprocessing_expert_tf_category_strategy_params\n",
    "def test_preprocessing_expert_get_possible_strategies_with_fit(default_preprocessing_expert_strategies,\n",
    "                                                               lending_club_data,\n",
    "                                                               cm_numerics_strategy, \n",
    "                                                               cm_category_strategy, \n",
    "                                                               tf_numerics_strategy, \n",
    "                                                               tf_category_strategy):\n",
    "    X, y = lending_club_data\n",
    "    data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "    \n",
    "    pe = PreprocessingExpert(correct_misses_numerics_strategy=cm_numerics_strategy, \n",
    "                             correct_misses_category_strategy=cm_category_strategy, \n",
    "                             transform_feat_numerics_strategy=tf_numerics_strategy, \n",
    "                             transform_feat_category_strategy=tf_category_strategy) \n",
    "        \n",
    "    pe.fit(X, y, data_report=data_report)\n",
    "    \n",
    "    possible_strategies = pe.get_possible_strategies()\n",
    "    assert possible_strategies == default_preprocessing_expert_strategies \n",
    "        \n",
    "\n",
    "@preprocessing_expert_cm_numerics_strategy_params_no_none\n",
    "@preprocessing_expert_cm_category_strategy_params_no_none\n",
    "@preprocessing_expert_tf_numerics_strategy_params_no_none\n",
    "@preprocessing_expert_tf_category_strategy_params_no_none\n",
    "def test_preprocessing_expert_fit_transform(lending_club_data,\n",
    "                                            cm_numerics_strategy, \n",
    "                                            cm_category_strategy, \n",
    "                                            tf_numerics_strategy, \n",
    "                                            tf_category_strategy):        \n",
    "    X, y = lending_club_data\n",
    "    data_report = ExploratoryDataAnalyst().analyze(X, y)\n",
    "\n",
    "    pe = PreprocessingExpert(correct_misses_numerics_strategy=cm_numerics_strategy, \n",
    "                             correct_misses_category_strategy=cm_category_strategy, \n",
    "                             transform_feat_numerics_strategy=tf_numerics_strategy, \n",
    "                             transform_feat_category_strategy=tf_category_strategy) \n",
    "        \n",
    "    pe.fit(X, y, data_report=data_report)\n",
    "        \n",
    "    expected_file_name = f'cm__{cm_numerics_strategy}-{cm_category_strategy}__ft__{tf_numerics_strategy}-{tf_category_strategy}.npz'\n",
    "    expected = np.load(expected_file_name, 'r')['arr_0']\n",
    "    \n",
    "    assert_array_equal(pe.transform(X), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def lending_club_categor_features2():\n",
    "    return [\n",
    "        'emp_length',\n",
    "        'home_ownership',\n",
    "        'verification_status',\n",
    "        'pymnt_plan',\n",
    "        'purpose_cat',\n",
    "        'zip_code',\n",
    "        'addr_state',\n",
    "        'initial_list_status',\n",
    "        'policy_code'\n",
    "    ]\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def new_unique_category():\n",
    "    return 'aee8eab3aeb0d2f20146ce8a4c1ebbb5'\n",
    "\n",
    "\n",
    "def test_fit_predict(random_state, lending_club_data):\n",
    "    X, y = lending_club_data\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "    \n",
    "    cls = LogRegClassifier(random_state=random_state)\n",
    "    cls.fit(X_train, y_train)\n",
    "    \n",
    "    pred = cls.predict(X_test)\n",
    "    expected = expected = np.load('LogRegClassifier_fit_predict.npz', 'r')['arr_0']\n",
    "        \n",
    "    assert_array_equal(pred, expected)\n",
    "        \n",
    "    \n",
    "def test_fit_predict_proba(random_state, lending_club_data):\n",
    "    X, y = lending_club_data\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "    \n",
    "    cls = LogRegClassifier(random_state=random_state)\n",
    "    cls.fit(X_train, y_train)\n",
    "    \n",
    "    pred_proba = cls.predict_proba(X_test)\n",
    "    expected = expected = np.load('LogRegClassifier_fit_predict_proba.npz', 'r')['arr_0']\n",
    "        \n",
    "    assert_array_equal(pred_proba, expected)\n",
    "\n",
    "    \n",
    "def test_evaluate(random_state, lending_club_data):\n",
    "    X, y = lending_club_data\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "    \n",
    "    cls = LogRegClassifier(random_state=random_state)\n",
    "    cls.fit(X_train, y_train)\n",
    "    \n",
    "    eval_res = cls.evaluate(X_test, y_test)\n",
    "                       \n",
    "    assert eval_res == {'f1_score': 0.21739130434782608, 'logloss': 0.3369127601475267}\n",
    "\n",
    "    \n",
    "def test_tune_parameters(random_state, lending_club_data):\n",
    "    X, y = lending_club_data\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "    \n",
    "    cls = LogRegClassifier(random_state=random_state)\n",
    "    \n",
    "    best_params = cls.tune_parameters(X_train, y_train)\n",
    "    eval_res = cls.evaluate(X_test, y_test)\n",
    "    \n",
    "    assert best_params == {\n",
    "        'best_parameters': {\n",
    "            'logit_classify__C': 0.1, \n",
    "            'preprocessings__correct_misses_category_strategy': 'most_frequent', \n",
    "            'preprocessings__correct_misses_numerics_strategy': 'median', \n",
    "            'preprocessings__transform_feat_category_strategy': 'ohe', \n",
    "            'preprocessings__transform_feat_numerics_strategy': 'standardize'\n",
    "        }, \n",
    "        'best_scores': {\n",
    "            'f1_score': 0.15587968850860037, \n",
    "            'logloss': 0.3573686967964508\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    assert eval_res == {'f1_score': 0.14155251141552513, 'logloss': 0.338917581762135}\n",
    "    \n",
    "\n",
    "def test_fit_predict_with_new_categories(random_state, new_unique_category, lending_club_data, lending_club_categor_features2):\n",
    "    X, y = lending_club_data\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random_state)\n",
    "\n",
    "    cls = LogRegClassifier(random_state=random_state)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    X_test = X_test.copy(deep=True)\n",
    "    \n",
    "    for c in lending_club_categor_features2:\n",
    "        X_test.loc[:,c] = new_unique_category\n",
    "\n",
    "    pred_proba = cls.predict_proba(X_test)\n",
    "    expected = np.load('LogRegClassifier_fit_predict_proba_with_new_categories.npz', 'r')['arr_0']\n",
    "\n",
    "    assert_array_equal(pred_proba, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hashlib\n",
    "\n",
    "# l =  [\n",
    "#         FeatureStats(name='emp_length', index=1, unique_num=14, missed_num=0), \n",
    "#         FeatureStats(name='home_ownership', index=2, unique_num=5, missed_num=0), \n",
    "#         FeatureStats(name='verification_status', index=4, unique_num=3, missed_num=0), \n",
    "#         FeatureStats(name='pymnt_plan', index=5, unique_num=2, missed_num=0), \n",
    "#         FeatureStats(name='purpose_cat', index=6, unique_num=27, missed_num=0), \n",
    "#         FeatureStats(name='zip_code', index=7, unique_num=720, missed_num=0), \n",
    "#         FeatureStats(name='addr_state', index=8, unique_num=50, missed_num=0), \n",
    "#         FeatureStats(name='initial_list_status', index=19, unique_num=2, missed_num=0), \n",
    "#         FeatureStats(name='policy_code', index=22, unique_num=5, missed_num=0)\n",
    "#     ]\n",
    "\n",
    "# cf = [f.name for f in l]\n",
    "# cf\n",
    "# X, y = lending_club_x, lending_club_y\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=242)\n",
    "\n",
    "# for c in cf:\n",
    "#     X_test[c] = 'aee8eab3aeb0d2f20146ce8a4c1ebbb5'\n",
    "\n",
    "# cls = LogRegClassifier(random_state=242)\n",
    "# cls.fit(X_train, y_train)\n",
    "\n",
    "# pred = cls.predict_proba(X_test)\n",
    "# np.savez_compressed('LogRegClassifier_fit_predict_proba_with_new_categories.npz', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................ [ 67%]\n",
      ".....................................................                                                            [100%]\n"
     ]
    }
   ],
   "source": [
    "ipytest.run('-qq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
