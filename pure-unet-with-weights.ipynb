{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The implementation of U-Net fully convolutional neural network\n",
    "Add weights along with image input tensor.  \n",
    "\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"  \n",
    "https://arxiv.org/pdf/1505.04597.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=241\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [26,19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_map_and_mask(map_file, mask_file, workspace_dir):\n",
    "    map_img = Image.open(str(map_file))\n",
    "    \n",
    "    polygons = []\n",
    "\n",
    "    with open(str(mask_file)) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            points = line.split(' ')\n",
    "            polygon = [(int(xy[0]), int(xy[1])) for xy in [point.split(',') for point in points]]\n",
    "            polygons.append(polygon)\n",
    "       \n",
    "    mask_img = Image.new('1', map_img.size, 0)\n",
    "\n",
    "    for p in polygons:\n",
    "        ImageDraw.Draw(mask_img).polygon(p, fill=1)\n",
    "    \n",
    "    mask_img.convert('RGB').save(str(workspace_dir/(mask_file.stem + '.jpg')), format='JPEG', quality=100)\n",
    "    \n",
    "    return map_img, mask_img  \n",
    "\n",
    "def plot_masks(map_imgs, mask_imgs, weights=None):\n",
    "    rows = len(map_imgs)\n",
    "    cols = 2 if weights is None else 3\n",
    "    \n",
    "    for i, m in enumerate(map_imgs):\n",
    "        plt.subplot(rows,cols, cols*i +1)\n",
    "        plt.imshow(m)\n",
    "\n",
    "        plt.subplot(rows,cols,cols*i +2)\n",
    "        plt.imshow(mask_imgs[i])\n",
    "        \n",
    "        if weights is not None:\n",
    "            plt.subplot(rows,cols,cols*i +3)\n",
    "            plt.imshow(weights[i])\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "def map_stats(map_img, mask_img):\n",
    "    m = np.array(mask_img).astype(np.byte)\n",
    "    n = np.sum(m == 0)\n",
    "    k = np.sum(m == 1)\n",
    "    \n",
    "    print(map_img.size)\n",
    "    print('zeros ratio', 0 if n == 0 else round(n/(n+k), 3))\n",
    "    print('ones ratio', 0 if k == 0 else round(k/(n+k), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = pathlib.Path().cwd() / 'data' / 'train'\n",
    "src_images = list(dataset_dir.glob('**/*.tif'))\n",
    "src_images = [img_p for img_p in src_images if '.mask.' not in img_p.name]\n",
    "\n",
    "print('src_images', len(src_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "workspace_dir = pathlib.Path().cwd() / ('workspace_' + str(timestamp))\n",
    "workspace_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read source Map images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_images_and_masks = []\n",
    "\n",
    "for img_p in src_images:\n",
    "    map_img, mask = load_map_and_mask(img_p, img_p.parent/(img_p.stem + '.markup.txt'), workspace_dir) \n",
    "    src_images_and_masks.append({\n",
    "        'map': map_img,\n",
    "        'mask': mask\n",
    "    })\n",
    "    \n",
    "    print('Stats for', img_p)\n",
    "    map_stats(map_img, mask)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masks(\n",
    "    [src['map'] for src in src_images_and_masks][:2], \n",
    "    [src['mask'] for src in src_images_and_masks][:2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create dataset images by cropping source Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import scipy.ndimage.morphology as morphology\n",
    "\n",
    "\n",
    "def create_weight_map(y, wc=None, w0 = 5, sigma = 30):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate weight maps accordin to the U-Net white-paper.\n",
    "        Parameters:\n",
    "            mask: numpy_array - array of shape (image_height, image_width) \n",
    "                                representing binary mask of objects\n",
    "            wc:   dict        - weight classes\n",
    "            w0:    int        - border weight\n",
    "            sigma: int        - border width\n",
    "\n",
    "        Returns:\n",
    "            numpy_array - weights of shape (image_height, image_width).\n",
    "    \"\"\"\n",
    "\n",
    "    labels = skimage.measure.label(y)\n",
    "    label_ids = sorted(np.unique(labels))[1:]\n",
    "    background_ids = labels == 0\n",
    "\n",
    "    if len(label_ids) > 1:\n",
    "        distances = np.zeros((y.shape[0], y.shape[1], len(label_ids)))\n",
    "\n",
    "        for i, label_id in enumerate(label_ids):\n",
    "            # https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.morphology.distance_transform_edt.html\n",
    "            distances[:,:,i] = morphology.distance_transform_edt(labels != label_id)\n",
    "\n",
    "        distances = np.sort(distances, axis=2)\n",
    "        d1 = distances[:,:,0]\n",
    "        d2 = distances[:,:,1]\n",
    "        w = w0 * np.exp(-1/2*((d1 + d2) / sigma)**2) * background_ids\n",
    "\n",
    "        if wc:\n",
    "            class_weights = np.zeros_like(y)\n",
    "            for k, v in wc.items():\n",
    "                class_weights[y == k] = v\n",
    "            w = w + class_weights\n",
    "    else:\n",
    "        w = np.zeros_like(y)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_map_into_tiles(map_img, \n",
    "                       mask_img, \n",
    "                       tile_size=100, \n",
    "                       tile_resize=100, \n",
    "                       tiles_count=100,\n",
    "                       tile_prefix='',\n",
    "                       save_tiles=True,\n",
    "                       save_dir=None):\n",
    "    X = []\n",
    "    Y = []\n",
    "    W = []\n",
    "\n",
    "    width, height = map_img.size\n",
    "    \n",
    "    top_left_coordinates = zip(\n",
    "        np.random.randint(0, width - tile_size, tiles_count), \n",
    "        np.random.randint(0, height - tile_size, tiles_count)\n",
    "    )\n",
    "\n",
    "    map_img_in_rgb = map_img.convert('RGB')\n",
    "    \n",
    "    for i, (x,y) in enumerate(top_left_coordinates):\n",
    "        tile = map_img_in_rgb.crop( (x, y, x+tile_size, y+tile_size) )\n",
    "        tile_mask = mask_img.crop( (x, y, x+tile_size, y+tile_size) )\n",
    "\n",
    "        tile = tile.resize((tile_resize, tile_resize))\n",
    "        tile_mask = tile_mask.resize((tile_resize, tile_resize))\n",
    "        \n",
    "        mp = np.array(tile)\n",
    "               \n",
    "        mask = np.array(tile_mask).astype(np.byte)\n",
    "        weights = create_weight_map(mask)\n",
    "        \n",
    "        X.append(mp)\n",
    "        Y.append(mask)\n",
    "        W.append(weights)\n",
    "        \n",
    "        if save_tiles:\n",
    "            np.save(str(save_dir/(tile_prefix + 'map_' + str(i) + '.np')), mp)\n",
    "            np.save(str(save_dir/(tile_prefix + 'mask_' + str(i) + '.np')), mask)\n",
    "            np.save(str(save_dir/(tile_prefix + 'weights_' + str(i) + '.np')), mask)\n",
    "        \n",
    "    return X, Y, W\n",
    "\n",
    "def tiles_stats(Y):\n",
    "    zeros_count = 0\n",
    "    ones_count = 0\n",
    "    for y in Y:\n",
    "        zeros_count += np.sum((y==0))\n",
    "        ones_count += np.sum((y==1))\n",
    "\n",
    "    print('zeros', zeros_count)\n",
    "    print('ones', ones_count)\n",
    "    \n",
    "    if zeros_count > 0:\n",
    "        print('zeros ratio', zeros_count/(ones_count + zeros_count))\n",
    "    if ones_count > 0:\n",
    "        print('ones ratio', ones_count/(ones_count + zeros_count))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILES_SIZE = 1024\n",
    "TILES_COUNT = 1\n",
    "UNET_INPUT_SIZE = 256\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "W = []\n",
    "\n",
    "tiles_folder = workspace_dir / 'train_tiles'\n",
    "tiles_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "for i, src in enumerate(src_images_and_masks):\n",
    "    x, y, w = cut_map_into_tiles(\n",
    "        src['map'], \n",
    "        src['mask'], \n",
    "        tile_size=TILES_SIZE,\n",
    "        tile_resize=UNET_INPUT_SIZE,\n",
    "        tiles_count=TILES_COUNT,\n",
    "        tile_prefix=str(i),\n",
    "        save_dir=tiles_folder\n",
    "    )\n",
    "    \n",
    "    X += x\n",
    "    Y += y\n",
    "    W += w\n",
    "    print('done', i)\n",
    "    \n",
    "print('X', len(X))\n",
    "print('Y', len(Y))\n",
    "print('W', len(W))\n",
    "tiles_stats(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 View cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_mask_to_img(data):\n",
    "    size = data.shape[::-1]\n",
    "    databytes = np.packbits(data, axis=1)\n",
    "    \n",
    "    return Image.frombytes(mode='1', size=size, data=databytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# View train data\n",
    "show_count = 10\n",
    "train_maps = [Image.fromarray(x.astype('uint8'), 'RGB') for x in X[:show_count]]\n",
    "train_masks = [binary_mask_to_img(y) for y in Y[:show_count]]\n",
    "train_weights = [w for w in W[:show_count]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [50,50]\n",
    "plot_masks(train_maps, train_masks, train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)[...,np.newaxis]\n",
    "W = np.array(W)\n",
    "\n",
    "print(X.shape, Y.shape, W.shape)\n",
    "\n",
    "np.save(str(workspace_dir/'X.np'), X)\n",
    "np.save(str(workspace_dir/'Y.np'), Y)\n",
    "np.save(str(workspace_dir/'W.np'), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create and train U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, LearningRateScheduler, ModelCheckpoint, EarlyStopping, \n",
    "                                        ReduceLROnPlateau, TensorBoard, TerminateOnNaN, Callback)\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet(input_sz=512):\n",
    "    image_input = Input(shape=(input_sz, input_sz, 3))\n",
    "    weights_input = Input(shape=(input_sz, input_sz))\n",
    "    \n",
    "# contracting path (down-sampling)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(image_input)\n",
    "    conv2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv6 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
    "\n",
    "    conv7 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv8 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv8)\n",
    "\n",
    "    conv9 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv10 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    \n",
    "# expansive path (up-sampling)\n",
    "    up_conv11 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(\n",
    "        UpSampling2D(size = (2,2))(conv10)\n",
    "    )\n",
    "    cancat1 = concatenate([conv8, up_conv11], axis = 3)\n",
    "    conv12 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(cancat1)\n",
    "    conv13 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "    \n",
    "    up_conv14 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(\n",
    "        UpSampling2D(size = (2,2))(conv13)\n",
    "    )\n",
    "    cancat2 = concatenate([conv6, up_conv14], axis = 3)\n",
    "    conv15 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(cancat2)\n",
    "    conv16 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv15)\n",
    "    \n",
    "    up_conv17 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(\n",
    "        UpSampling2D(size = (2,2))(conv16)\n",
    "    )\n",
    "    cancat3 = concatenate([conv4, up_conv17], axis = 3)\n",
    "    conv18 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(cancat3)\n",
    "    conv19 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv18)\n",
    "\n",
    "    up_conv20 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(\n",
    "        UpSampling2D(size = (2,2))(conv19)\n",
    "    )\n",
    "    cancat4 = concatenate([conv2, up_conv20], axis = 3)\n",
    "    conv21 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(cancat4)\n",
    "    conv22 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv21)\n",
    "    conv22 = SpatialDropout2D(0.2)(conv22)\n",
    "    \n",
    "    conv23 = Conv2D(1, 1, 1, activation = 'sigmoid')(conv22)\n",
    "       \n",
    "    return Model(inputs = [image_input, weights_input], outputs = conv23), weights_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet, weights_tensor = create_unet(input_sz=UNET_INPUT_SIZE)\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(X):\n",
    "    return (2.0 / 255.0) * X - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preprocessed = preprocess_inputs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "SMOOTH = 1\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/613aeff37a721450d94906df1a3f3cc51e2299d4/keras/backend/tensorflow_backend.py#L3626\n",
    "def weighted_bce(y_true, y_pred, weights, sample_weight=None):\n",
    "    bce = binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(weights*bce)\n",
    "\n",
    "def jaccard_score(gt, pr, smooth=SMOOTH, threshold=None):\n",
    "    \"\"\" \n",
    "        Jaccard index: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    Args:\n",
    "        gt: ground truth 4D keras tensor (B, H, W, C)\n",
    "        pr: prediction 4D keras tensor (B, H, W, C)\n",
    "        smooth: value to avoid division by zero\n",
    "        threshold: value to round predictions (use `>` comparison), \n",
    "                   if `None` prediction prediction will not be round\n",
    "    Returns:\n",
    "        IoU/Jaccard score in range [0, 1]\n",
    "    \"\"\"\n",
    "    axes = [1, 2]\n",
    "        \n",
    "    if threshold is not None:\n",
    "        pr = K.greater(pr, threshold)\n",
    "        pr = K.cast(pr, K.floatx())\n",
    "\n",
    "    intersection = K.sum(gt * pr, axis=axes)\n",
    "    union = K.sum(gt + pr, axis=axes) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "\n",
    "    iou = K.mean(iou, axis=0)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_callbacks(workspace_dir, batch_sz=1):\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    checkpoint_folder = workspace_dir / 'checkpoints' / str(timestamp)\n",
    "    checkpoint_folder.mkdir(parents=True)\n",
    "    tensorboard_folder = workspace_dir / 'tensorboard_logs' / str(timestamp)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        str(checkpoint_folder / 'model-{loss:.2f}.h5'),\n",
    "        monitor='loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='auto',\n",
    "        period=1\n",
    "    )\n",
    "    \n",
    "    stop = EarlyStopping(monitor='loss', patience=200, mode='min', verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-9, verbose=1, mode='min')\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=str(tensorboard_folder),\n",
    "                              histogram_freq=0,\n",
    "                              batch_size=batch_sz,\n",
    "                              write_graph=False, \n",
    "                              write_grads=False, \n",
    "                              write_images=False,\n",
    "                              embeddings_freq=0, \n",
    "                              embeddings_layer_names=None, \n",
    "                              embeddings_metadata=None, \n",
    "                              embeddings_data=None)\n",
    "    \n",
    "    return [reduce_lr, TerminateOnNaN(), checkpoint, tensorboard], checkpoint_folder\n",
    "\n",
    "\n",
    "def train(model, weights_tensor, X_train, Weights, Y_train, workspace_dir, epochs=1, batch_sz=1):\n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.99, nesterov=True)\n",
    "    \n",
    "    unet.compile(\n",
    "        #optimizer=sgd, \n",
    "        optimizer='Adam',\n",
    "        loss=partial(weighted_bce, weights=weights_tensor),\n",
    "        metrics=[jaccard_score, 'binary_accuracy']\n",
    "    )\n",
    "        \n",
    "    callbacks, checkpoint_dir = create_default_callbacks(workspace_dir, batch_sz=batch_sz)\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    with open(str(checkpoint_dir/'graph.json'), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "       \n",
    "    return model.fit(\n",
    "        [X_train, Weights], Y_train,\n",
    "        batch_size=batch_sz,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    unet, \n",
    "    weights_tensor, \n",
    "    X_preprocessed, \n",
    "    W, \n",
    "    Y, \n",
    "    workspace_dir, \n",
    "    epochs=700, \n",
    "    batch_sz=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Check the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "final_model_path = workspace_dir / 'checkpoints' / '2019-06-26-08-44-13' / 'model-0.23.h5'\n",
    "\n",
    "with open(str(final_model_path.parent / 'graph.json'), 'r') as json_file:\n",
    "    fitted_model = model_from_json(json_file.read())\n",
    "    \n",
    "fitted_model.load_weights(str(final_model_path), by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Use test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for i, src in enumerate(src_images_and_masks):\n",
    "    x, y = cut_map_into_tiles(\n",
    "        src['map'], \n",
    "        src['mask'], \n",
    "        tile_size=TILES_SIZE,\n",
    "        tile_resize=UNET_INPUT_SIZE,\n",
    "        tiles_count=10,\n",
    "        tile_prefix=str(i),\n",
    "        save_tiles=False\n",
    "    )\n",
    "    \n",
    "    X_test += x\n",
    "    Y_test += y\n",
    "    print('done', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_grayscale_data_to_red_rgba(mask, alpha_value=50):\n",
    "    data = (mask * 255).astype('uint8')\n",
    "    alpha = (mask * alpha_value).astype('uint8')\n",
    "\n",
    "    data = data.reshape((data.shape[0], data.shape[1], 1))\n",
    "    npad = ((0, 0), (0, 0), (0, 2))\n",
    "    rgba_array = np.pad(data, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "    rgba_array = np.insert(\n",
    "        rgba_array,\n",
    "        3,\n",
    "        alpha,\n",
    "        axis=2\n",
    "    )\n",
    "    return Image.fromarray(rgba_array, 'RGBA')\n",
    "\n",
    "def apply_predicted_mask(orig_image, predicted_2d_values):\n",
    "    predicted_img = convert_grayscale_data_to_red_rgba(predicted_2d_values)\n",
    "\n",
    "    orig_image = Image.fromarray(orig_image.astype('uint8'), 'RGB')\n",
    "    orig_image = orig_image.convert('RGBA')\n",
    "    orig_image.paste(predicted_img, (0, 0), predicted_img)\n",
    "\n",
    "    return orig_image.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(X_test, Y_test):\n",
    "    x_np = preprocess_inputs(np.array(x))\n",
    "    predicted = fitted_model.predict(x_np[np.newaxis,:,:,:])\n",
    "    predicted_2d = predicted.reshape((predicted.shape[1], predicted.shape[2]))\n",
    "    \n",
    "    display(apply_predicted_mask(x, predicted_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
